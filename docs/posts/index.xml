<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Clericpy&#39;s Blog</title>
    <link>https://clericpy.github.io/blog/posts/</link>
    <description>Recent content in Posts on Clericpy&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Jan 2022 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://clericpy.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>周报拾遗(持续更新)</title>
      <link>https://clericpy.github.io/blog/posts/%E5%91%A8%E6%8A%A5%E6%8B%BE%E9%81%97/</link>
      <pubDate>Sun, 02 Jan 2022 00:00:00 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/%E5%91%A8%E6%8A%A5%E6%8B%BE%E9%81%97/</guid>
      <description>将会按时间倒序, 不定期更新一些周报里读到的好东西, 并加入一点简评. 每次修改会顺带更新日期, 所以会跑到列表最前面.
 2022年1月2日 Writing fast async HTTP requests in Python | JonLuca’s Blog   主要看了 aiohttp 参数调优
  conn = aiohttp.TCPConnector(limit=None, ttl_dns_cache=300)
 延长 dns 缓存7有效期    http.cookies._is_legal_key = lambda _: True
 貌似可以解决总是在 stderr 里提示 illegal Cookie key 的问题, 这东西很烦      LibreTranslate  完全离线自托管的翻译服务, 平时偶尔用一下应该没有问题, 底层是 Argos Translate  GitHub - Nuitka/Nuitka: Nuitka is a Python compiler written in Python.</description>
    </item>
    
    <item>
      <title>再谈 Python 的打包与发布『zipapps』</title>
      <link>https://clericpy.github.io/blog/posts/%E5%86%8D%E8%B0%88-python-%E7%9A%84%E6%89%93%E5%8C%85%E4%B8%8E%E5%8F%91%E5%B8%83/</link>
      <pubDate>Fri, 07 May 2021 22:10:51 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/%E5%86%8D%E8%B0%88-python-%E7%9A%84%E6%89%93%E5%8C%85%E4%B8%8E%E5%8F%91%E5%B8%83/</guid>
      <description>我希望把 Python 程序打包给别人用的时候像普通软件一样简单
 接上回 入职新公司之后遇到 Python 打包发布的问题, 由于基础建设以及技术选型等历史遗留问题, 只能使用 Python 写的工具, 所以基本告别 pipenv / venv / poetry 等常见的发布方式, 遇到需要第三方依赖的功能, 打包依赖变得非常费劲.
考虑过像 go 一样打包一些二进制文件, 但是性能和兼容性等方面特别不友好, 所以还是打算使用 zipapp 那种方式来搞. pyinstaller / venv / pipenv / poetry / setuptools / shiv / PyOxidizer 这些的使用体验, 其实本身还是不错的.
新的选择 目前这类比较成熟的几个打包用的库里面, shiv 算是生态还算健康的选择, 类似的 pex / superzippy 使用过程中有挺多缓存或者其他不方便的地方, 所以用了大概三个多月 shiv (https://github.com/linkedin/shiv), 期间挺多小问题能解决的都解决了, 不能解决的提 issue 作者也比较快地支持了.
然而最后还是选择自己开发个类似项目 zipapps, 因为下面几个原因 shiv 支持的不是很方便:
 不能在打包时候设置默认 SHIV_ROOT 默认推到 HOME 目录在某些场景缺少 HOME 会报错, 每次手动定义这个环境变量又感觉很冗余  提过 issue 以后目前支持上了   每次重新打包以后要重新创建缓存, 时间久了导致系统磁盘浪费几 GB 空间, 其实这是最主要的那个原因  有别人提了 build_id 的建议, 貌似没通过   每次需要解压全部内容做缓存  实际上 zipapp 借助 zipimport 自带能力, 有的库是不需要解压的, 这样可以保证发布时的 standalone 特性   跨平台和跨 Python 版本能力比较薄弱  比如 Python3.</description>
    </item>
    
    <item>
      <title>文件夹图片一键生成 HTML 相册</title>
      <link>https://clericpy.github.io/blog/posts/%E5%B0%86%E6%9C%AC%E5%9C%B0%E6%BC%AB%E7%94%BB%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%BD%AC%E4%B8%BA-html/</link>
      <pubDate>Mon, 04 May 2020 16:42:54 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/%E5%B0%86%E6%9C%AC%E5%9C%B0%E6%BC%AB%E7%94%BB%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%BD%AC%E4%B8%BA-html/</guid>
      <description>因为代码量比较小, 暂时不打算上 pypi
 下载地址:
https://github.com/ClericPy/somethings/releases
  源码 (建议): https://github.com/ClericPy/somethings/blob/master/Python/pic2html.pyw
 直接复制源代码更干净, 因为没有第三方依赖, 不过需要 python 解释器环境, windows 下双击即可运行
   单文件 exe: pic2html-single.exe
 每次执行会产生临时文件, 执行结束会自动清理
   文件夹 exe: pic2html.zip
 不产生临时文件, 不过需要解压缩
   场景 日常生活中, 总有需要浏览大量图片的时候, 比如打包的 漫画 / 壁纸 / 相片 / 素材, 浏览图片集过程中, 也会产生一些常见的需求:
 图片放大拉宽, 方便文字阅读 能快速定位到文件的路径和名称 一次加载多张图片, 免得手动一张张翻页 支持多种图片格式 单文件 exe 无需安装  用法  手动复制要处理的文件夹路径 执行脚本, 会自动从剪切板中读取路径  可能是 pyw 文件, 也可能是 exe 文件   等待执行结束, 自动打开浏览器查看  双击图片则打开单个图片方便单独放大 顶部调整图片宽度占比 顶部锚点链接负责跳转到指定文件夹路径  单个文件夹标题点击会回到顶部   每张图片的右下角会有小字显示文件名和路径   HTML 文件会放在剪切板路径中, 命名以 - 开头  细节   HTML 文件要与文件夹放在一起, 不留冗余文件</description>
    </item>
    
    <item>
      <title>随手试了下 B 站的滑块验证码</title>
      <link>https://clericpy.github.io/blog/posts/20200402142224/</link>
      <pubDate>Thu, 02 Apr 2020 14:22:24 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20200402142224/</guid>
      <description>没什么意思, 就是随手试试, 记录下代码, 顺便看看 cdp 代替 selenium
  特意截了个先失败一次的图 https://clericpy.github.io/blog/images/bilibili_demo.gif
 思路  根据目标方框位置的边角特征, 投票选出最可能的左边界  特征图包括左侧边/左上角/左下角得到左边界, 右侧边/右上角/右下角剪掉方框宽度 42px 得到右边界 特征图比较小 (2 * 5 左右), 直接用 b64 就够了 左边界结果还要额外 -5, 因为滑块并不在 0 的位置 按照 3px 的误差分组, 间隔 3px 以内的算一组, 最后取中位数作为该组结果 按组投票, 票数最多的三组最接近实际结果, 按顺序一个个拖拽即可   如果三组偏移拖完依然没成功, 则刷新页面重新来过. 考虑到拖拽时候的拟人轨迹, 使用的是 pyautogui, 所以要前台显示窗口  用 chrome cdp 里其实也有, https://chromedevtools.github.io/devtools-protocol/tot/Input#method-dispatchMouseEvent , 但是这是基于发送鼠标事件的方式来调用的, 并没有移动鼠标的实际位置, 基本 100% 被检测到. 通过 pyautogui 被当作机器的可能比较小, 速度也很快, 用 cdp 模拟拖拽事件虽然支持 Headless 后台拖拽, 却基本都是被识别为机器, 这部分需要伪造人类拖拽轨迹.</description>
    </item>
    
    <item>
      <title>watchdogs: 随时关注网络世界的变化</title>
      <link>https://clericpy.github.io/blog/posts/20200331171211/</link>
      <pubDate>Tue, 31 Mar 2020 17:12:12 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20200331171211/</guid>
      <description>做爬虫的, 要有自己的工具来发现网络世界的动态, 对信息保持一份敏感, 对数据存有一颗敬畏之心.
 项目名称: ClericPy/watchdogs
项目初衷   统一关注.
 平时可能会追剧追小说追综艺, 关注疫情实况, 订阅一些博客, 但是网站多了太分散, 所以需要一份 Timeline 方便浏览.
   解放双手.
 满足对日常信息变化的订阅, 并提供主动提醒功能, 从不断刷网页判断更新中解放双手. 简而言之, 信息索取由被动转为主动.
   体验新事物.
 而归根结底的初衷, 还是想试试去年就大火的 fastapi, 体验之后, 果然比原生 starlette 要优雅很多.
   主要功能  适用于: 追剧, 追星, 追小说漫画, steam 降价, 博客更新, 服务翻车, 疫情实况, 定向新闻或搜索结果等场景的提醒. 根据定制解析规则, 关注网页特定数据的变化, 记录更新时间与历史数据, 并提供对应 URL 进行跳转. 所有爬虫执行在协程环境下, 抓取 / 存储 / 回调与整个 Web 服务相互独立不阻塞.</description>
    </item>
    
    <item>
      <title>深入浅出 CDP (Chrome DevTools Protocol)</title>
      <link>https://clericpy.github.io/blog/posts/20200114151137/</link>
      <pubDate>Tue, 14 Jan 2020 15:11:38 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20200114151137/</guid>
      <description>背景 ​	自从 Chrome 59 发布支持 &amp;ndash;headless 启动参数以后 (Windows 上是 60 版本), 轻量级浏览器内核就不再是 webdriver 一家独大, 甚至 phantomjs 作者也发文表示不再维护该项目, 国外也有越来越多的文章推荐使用 headless Chrome 代替过去 selenium + webdriver 的方式进行 Web 测试或者爬虫相关工作.目前国内实际上使用 headless Chrome 的并不少, 只不过目前大量营销号的存在, 导致为了点击量频繁刷文, 进而把早年间 selenium 用作爬虫的旧文章重新翻到读者眼前, 所以遇到各种稀奇古怪的问题, 初学者使用体验较差.selenium 作为老牌 Web 测试手段闻名已久, 在高级功能 API 层面非常成熟, 后来也加强了对 Chrome headless 模式下 CDP 的支持, 目前依然拥有大量用户在使用.这里, 简单提一下 selenium + webdriver 方式的一些不足:
 默认参数启动时很容易被服务端发现 性能与 Chrome headless 相比, 较差 存在了无数年的内存泄漏问题 不像 Chrome 有大厂在背后支撑, 上千 issues 解决不完 无法作为完整浏览器使用和调试  ​	简而言之, 都 2020 年了, 不要再抱着 selenium 不放了</description>
    </item>
    
    <item>
      <title>闲谈 Python 打包发布</title>
      <link>https://clericpy.github.io/blog/posts/20191221143647/</link>
      <pubDate>Sat, 21 Dec 2019 14:36:47 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20191221143647/</guid>
      <description>1 发布到 pypi setup.py 这套东西已经很成熟了, 不再赘述, 当前主流的是 twine + wheel 的方式, 好处很简单, 就是 pip install 安装方式省心省力目前也比较流行 poetry 这个库的 publish 功能进行发布
2 作为 pyz 发布 Shiv 有 PEP441 的加成, 目前 Shiv 在这方面做的已经非常熟练了:
 通过 pip 收集所需要的第三方库 通过 shebang 来指定 Python 解释器路径 通过 zipapp 相关功能把整个 package 打包成一个 pyz 压缩文件(实质是zip文件换了个扩展名) 通过 _bootstrap 指定执行条件 module:app 通过命令行执行整个打包过程  优点:  单文件上传下载简单方便 可以直接使用 python xxx.pyz 来执行 package 里的 main.py 也可以将 xxx.pyz 当作一个环境, 来执行其它的 py 脚本, 比如 python xxx.</description>
    </item>
    
    <item>
      <title>设计模式拾遗</title>
      <link>https://clericpy.github.io/blog/posts/20191221001405/</link>
      <pubDate>Sat, 21 Dec 2019 00:14:06 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20191221001405/</guid>
      <description>没有什么 &amp;ldquo;新瓶装旧酒&amp;rdquo;, 只希望温故而知新吧
 快速回顾, 具体细节不多讲, 可能有理解不对的地方, 请留言指正
示例代码: Python
设计模式简介 1 简单工厂 Simple Factory &amp;ldquo;工厂&amp;rdquo; 顾名思义就是负责生产产品(对象)的地方, 简单工厂则主要处理单一产品等级结构的情况由一个工厂类来决定创建哪个 / 哪些对象, 将创建对象的操作集中在一起, 使之与对象的使用解耦将对象的创建与使用解耦, 也就是俗称的:** **要什么就造什么现实案例: 宠物店新增宠物
class AnimalFactory(object): &amp;#34;&amp;#34;&amp;#34;动物工厂, 负责产生动物&amp;#34;&amp;#34;&amp;#34; @staticmethod def create_animal(name): if name == &amp;#39;Cat&amp;#39;: return Cat() elif name == &amp;#39;Dog&amp;#39;: return Dog() def main(): dog = AnimalFactory.create_animal(&amp;#39;Dog&amp;#39;) cat = AnimalFactory.create_animal(&amp;#39;Cat&amp;#39;) if __name__ == &amp;#34;__main__&amp;#34;: main() 2 工厂方法 Factory Method 基类负责抽象一个工厂方法(接口), 而不用关心具体如何实现, 具体实现让子类来负责现实案例: 动物园新增动物
import abc class AnimalFactory(metaclass=abc.ABCMeta): &amp;#34;&amp;#34;&amp;#34;相当于定义了一个接口, 所有类型的工厂都要继承这个类, 并覆盖抽象方法 create_animal&amp;#34;&amp;#34;&amp;#34; @abc.</description>
    </item>
    
    <item>
      <title>回顾软件设计的原则</title>
      <link>https://clericpy.github.io/blog/posts/20191212204823/</link>
      <pubDate>Thu, 12 Dec 2019 20:48:24 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20191212204823/</guid>
      <description>鉴于最近又写出大量低质量代码, 所以需要重新回顾一下这些有用的编码哲学.
 主要参照 &amp;lt;一些软件设计的原则&amp;gt; &amp;ndash; 酷壳, 做一下简单理解的笔记.
1. 一句话总结:  大段重复代码要提取函数, 方便复用和统一修改 高层依赖抽象, 抽象去依赖底层实现. 因为高层抽象逻辑很少变, 而底层实现却经常要变 功能之间相互依赖越少越好, 逻辑越独立越好, 不要过度优化和过早优化 底层的每个函数职责尽量单一, 每个类只做好一件事, 功能粒度越细越好  无状态无副作用的纯函数是捷径   多用接口少用继承. 由上层抽象来统一组合调配, 进而实现完整功能  2. 设计原则明细 面向对象的S.O.L.I.D 原则  Single Responsibility Principle (SRP) – 职责单一原则  一个&amp;quot;类&amp;quot;只做好一件事, 一个函数只实现一个功能 纯面向过程写代码会导致逻辑不够清晰, 更容易出错   Open/Closed Principle (OCP) – 开闭原则  依赖抽象，而不是实现 对扩展是开放的，而对修改是封闭的  一方面要保证业务层对底层具体实现相隔离, 避免随意修改导致原有功能遭到破坏 另一方面避免修改底层代码而导致未知引用遭到破坏, 产生不期结果     Liskov Substitution Principle (LSP) – 里氏替换原则  任何基类可以出现的地方，子类一定可以出现 2.</description>
    </item>
    
    <item>
      <title>Python 中文教程在 2019 年的选择</title>
      <link>https://clericpy.github.io/blog/posts/20191027113439/</link>
      <pubDate>Sun, 27 Oct 2019 11:34:40 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20191027113439/</guid>
      <description>本文不会太大篇幅讲东西, 只是把必要信息罗列一下, 节省 Python 入门时间, 少走弯路.
 前言 Python 的入门中文教程, 如果放在三四年前, 那可以选择的太多了
 廖雪峰 Python3 教程  那个年头确实最好的选择  在线阅读 保持更新新特性 作者知识面比较广     Python 之旅  这个是很后期才看到的, 质量意外的不错   Head First Python  我当年买的入门第一本书, 实际上看完更迷了&amp;hellip;   Learn Python the Hard Way (笨方法学 Python)  曾经稍微看过, 只能说类似于应试教育的刷题培养肌肉记忆的好书   Think Python  图书馆借来看的, 有点旧, 信息量也不算大 但是内容质量非常高   陪孩子学 Python 之类的儿童读物  这种东西&amp;hellip; 不是说不好, 标题让我讨厌   各种网站上的视频教程  说实话, 我真正培养入门兴趣的还正是 B 站已经有的小甲鱼零基础学 Python 视频 视频类型的不枯燥, 有些老师还会讲段子发散思维便于记忆 但是一个视频的信息量真的太少了, 时间充裕的可以试试    但进入 2019 年以来, 以上教程基本已经不算第一梯队的选择了.</description>
    </item>
    
    <item>
      <title>爬虫常用代码片段合集 - [torequests.utils]</title>
      <link>https://clericpy.github.io/blog/posts/20191025183222/</link>
      <pubDate>Fri, 25 Oct 2019 18:32:22 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20191025183222/</guid>
      <description>前言 此处省略 150 字引言, 假装以后会补.
正文 以下功能均已收入 torequests
 pip install torequests -U
   查看当前进程的内存占用. 借助 psutil
  代码
def print_mem(unit=&amp;#34;MB&amp;#34;): &amp;#34;&amp;#34;&amp;#34;Show the proc-mem-cost with psutil, use this only for lazinesssss. :param unit: B, KB, MB, GB. &amp;#34;&amp;#34;&amp;#34; try: import psutil, os B = float(psutil.Process(os.getpid()).memory_info().vms) KB = B / 1024 MB = KB / 1024 GB = MB / 1024 result = vars()[unit] print(&amp;#34;memory usage: %.2f(%s)&amp;#34; % (result, unit)) return result except ImportError: print(&amp;#34;pip install psutil first.</description>
    </item>
    
    <item>
      <title>通过 Python &#43; Chrome cURL 重放 HTTP 请求</title>
      <link>https://clericpy.github.io/blog/posts/20191024220923/</link>
      <pubDate>Thu, 24 Oct 2019 22:09:23 +0800</pubDate>
      
      <guid>https://clericpy.github.io/blog/posts/20191024220923/</guid>
      <description>众所周知, 一条完整 cURL 命令, 基本可以完整涵盖一次 HTTP 请求的全部信息, 所以, 只要有一句 Curl 命令, 就足以向服务器发起一次完全一样的请求.
 一个 HTTP 请求有多常见?  百度知道答案的点赞 斗鱼直播的一次弹幕 github issues 的一次评论  换句话说, 只要拿到一段 cURL 命令, 就能够将类似上面的各路请求再次复现.
更进一步地说, 既然拿到了命令, 略微修改里面的参数便足以发起与上次请求内容相异的同类请求.
手把手做个实验 实验准备   安装 Python, 并搞一个可以解析 cURL 的库
 pip install torequests -U
torequests &amp;gt;= 4.8.18, 刚修复了一个 \n 导致提交出错的 bug
   安装一个 Chrome 浏览器
  实验步骤 from torequests.utils import curlparse import requests curlstring = r&amp;#39;&amp;#39;&amp;#39;curl &amp;#39;https://github.</description>
    </item>
    
  </channel>
</rss>